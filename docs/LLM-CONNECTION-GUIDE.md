# LLM Connection Guide

Complete guide for connecting to the AI Inference Platform's LLM backend.

## Table of Contents

- [Overview](#overview)
- [Connection Details](#connection-details)
- [API Endpoints](#api-endpoints)
- [Authentication](#authentication)
- [Available Models](#available-models)
- [Usage Examples](#usage-examples)
- [Integration Methods](#integration-methods)
- [Troubleshooting](#troubleshooting)

## Overview

The AI Inference Platform provides an OpenAI-compatible API for accessing multiple LLM models. The platform supports:

- **OpenAI SDK** - Drop-in replacement for OpenAI's Python SDK
- **REST API** - Standard HTTP/JSON API
- **Web UI** - Browser-based interface (Open WebUI)
- **cURL** - Command-line testing

### Architecture

```
Client Application
       ↓
  [API Gateway]
       ↓
Azure Functions (Orchestrator)
       ↓
  [Model Router]
       ↓
GPU Inference Backends
```

## Connection Details

### Getting Your Connection Details

Run the setup script or retrieve from Terraform:

```bash
# Option 1: Run setup script (saves to connection-details.txt)
./setup-frontend-complete.sh

# Option 2: Get from Terraform
cd terraform
terraform output frontend_url
terraform output -raw resource_group_name
cd ..

# Get backend URL
az functionapp show --name <function-app-name> --resource-group <rg> --query defaultHostName -o tsv

# Get API key
az keyvault secret show --vault-name <key-vault-name> --name "frontend-openai-api-key" --query value -o tsv
```

### Connection Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| **Base URL** | `https://<your-app>.azurewebsites.net/api/v1` | Backend API endpoint |
| **API Key** | Retrieved from Key Vault | Authentication token |
| **Frontend URL** | `https://<your-app>.azurecontainer.io` | Web UI endpoint |
| **Models** | `mixtral-8x7b`, `llama-3-70b`, `phi-3-mini` | Available models |

## API Endpoints

### Base URL

```
https://<function-app-name>.azurewebsites.net/api/v1
```

### Endpoints

#### Chat Completions

```
POST /chat/completions
```

Create a chat completion response.

**Headers:**
```
Content-Type: application/json
Authorization: Bearer <api-key>
```

**Request Body:**
```json
{
  "model": "mixtral-8x7b",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 256,
  "top_p": 1.0,
  "stream": false
}
```

**Response:**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "mixtral-8x7b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

#### List Models

```
GET /models
```

List available models.

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "mixtral-8x7b",
      "object": "model",
      "created": 1700000000,
      "owned_by": "mistralai",
      "context_length": 32768,
      "pricing": {"prompt": 0.002, "completion": 0.002}
    },
    ...
  ]
}
```

#### Health Checks

```
GET /health          # Overall health
GET /health/live     # Liveness probe
GET /health/ready    # Readiness probe
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-17T00:00:00Z",
  "checks": {
    "cache": "ok",
    "secrets": "ok"
  }
}
```

## Authentication

### API Key Authentication

All requests require an API key in the `Authorization` header:

```bash
Authorization: Bearer <your-api-key>
```

### Retrieving Your API Key

```bash
# From Key Vault
az keyvault secret show \
  --vault-name <key-vault-name> \
  --name "frontend-openai-api-key" \
  --query value -o tsv

# Or from connection-details.txt (generated by setup script)
cat connection-details.txt | grep "API Key"
```

### Security Best Practices

- ✅ Store API key in environment variables
- ✅ Never commit API keys to version control
- ✅ Rotate keys periodically
- ✅ Use HTTPS for all connections
- ✅ Limit API key access to trusted applications

## Available Models

### Model Specifications

#### mixtral-8x7b

```yaml
Model ID: mixtral-8x7b
Provider: Mistral AI
Context Length: 32,768 tokens
Best For: Long documents, multi-turn conversations
Speed: Fast
Cost: $0.002 per 1K tokens
```

**Use Cases:**
- Document analysis
- Code generation
- Long conversations
- Research tasks

#### llama-3-70b

```yaml
Model ID: llama-3-70b
Provider: Meta
Context Length: 8,192 tokens
Best For: High-quality responses, complex reasoning
Speed: Medium
Cost: $0.003 per 1K tokens
```

**Use Cases:**
- Complex reasoning
- Creative writing
- Technical explanations
- Expert consultations

#### phi-3-mini

```yaml
Model ID: phi-3-mini
Provider: Microsoft
Context Length: 4,096 tokens
Best For: Quick queries, simple tasks
Speed: Very Fast
Cost: $0.0005 per 1K tokens
```

**Use Cases:**
- Quick questions
- Simple translations
- Basic summaries
- Testing/development

### Model Selection

```python
# Auto-select based on context length (recommended)
response = client.chat.completions.create(
    model="auto",  # Platform selects optimal model
    messages=[...]
)

# Explicit model selection
response = client.chat.completions.create(
    model="mixtral-8x7b",  # Force specific model
    messages=[...]
)
```

## Usage Examples

### Python (OpenAI SDK)

#### Installation

```bash
pip install openai
```

#### Basic Usage

```python
from openai import OpenAI

# Initialize client
client = OpenAI(
    api_key="your-api-key-here",
    base_url="https://your-app.azurewebsites.net/api/v1"
)

# Create chat completion
response = client.chat.completions.create(
    model="mixtral-8x7b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ],
    temperature=0.7,
    max_tokens=256
)

print(response.choices[0].message.content)
```

#### Streaming Response

```python
response = client.chat.completions.create(
    model="mixtral-8x7b",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

#### Multi-turn Conversation

```python
messages = [
    {"role": "system", "content": "You are a coding assistant."}
]

# First message
messages.append({"role": "user", "content": "Write a Python function to sort a list"})
response = client.chat.completions.create(model="mixtral-8x7b", messages=messages)
messages.append({"role": "assistant", "content": response.choices[0].message.content})

# Follow-up
messages.append({"role": "user", "content": "Now add error handling"})
response = client.chat.completions.create(model="mixtral-8x7b", messages=messages)
print(response.choices[0].message.content)
```

### cURL

#### Simple Request

```bash
curl -X POST https://your-app.azurewebsites.net/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "mixtral-8x7b",
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7,
    "max_tokens": 256
  }'
```

#### With System Prompt

```bash
curl -X POST https://your-app.azurewebsites.net/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "llama-3-70b",
    "messages": [
      {"role": "system", "content": "You are a Python expert."},
      {"role": "user", "content": "Explain list comprehensions"}
    ],
    "temperature": 0.5,
    "max_tokens": 512
  }'
```

#### List Models

```bash
curl https://your-app.azurewebsites.net/api/v1/models \
  -H "Authorization: Bearer your-api-key"
```

### JavaScript/Node.js

```javascript
const OpenAI = require('openai');

const client = new OpenAI({
  apiKey: 'your-api-key',
  baseURL: 'https://your-app.azurewebsites.net/api/v1'
});

async function chat() {
  const response = await client.chat.completions.create({
    model: 'mixtral-8x7b',
    messages: [
      { role: 'user', content: 'Hello!' }
    ],
    temperature: 0.7,
    max_tokens: 256
  });
  
  console.log(response.choices[0].message.content);
}

chat();
```

### Go

```go
package main

import (
    "context"
    "fmt"
    "github.com/sashabaranov/go-openai"
)

func main() {
    config := openai.DefaultConfig("your-api-key")
    config.BaseURL = "https://your-app.azurewebsites.net/api/v1"
    client := openai.NewClientWithConfig(config)
    
    resp, err := client.CreateChatCompletion(
        context.Background(),
        openai.ChatCompletionRequest{
            Model: "mixtral-8x7b",
            Messages: []openai.ChatCompletionMessage{
                {
                    Role:    openai.ChatMessageRoleUser,
                    Content: "Hello!",
                },
            },
        },
    )
    
    if err != nil {
        panic(err)
    }
    
    fmt.Println(resp.Choices[0].Message.Content)
}
```

## Integration Methods

### Web UI (Open WebUI)

The platform includes a browser-based interface:

1. **Access**: Navigate to frontend URL
2. **Login**: Use your admin credentials
3. **Select Model**: Choose from dropdown
4. **Chat**: Type your message and send
5. **Settings**: Adjust temperature, tokens, etc.

See [frontend-usage.md](frontend-usage.md) for detailed guide.

### REST API

Direct HTTP/JSON API calls:

- **Language**: Any language with HTTP support
- **Format**: JSON request/response
- **Auth**: Bearer token
- **Protocol**: HTTPS

### OpenAI SDK

Drop-in replacement for OpenAI:

```python
# Standard OpenAI code
from openai import OpenAI
client = OpenAI()  # Uses OpenAI

# Change to your platform
client = OpenAI(
    api_key="your-key",
    base_url="your-url"  # Only difference!
)
```

### LangChain Integration

```python
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    model_name="mixtral-8x7b",
    openai_api_key="your-api-key",
    openai_api_base="https://your-app.azurewebsites.net/api/v1",
    temperature=0.7
)

response = llm.predict("What is 2+2?")
print(response)
```

### LlamaIndex Integration

```python
from llama_index import OpenAI

llm = OpenAI(
    model="llama-3-70b",
    api_key="your-api-key",
    api_base="https://your-app.azurewebsites.net/api/v1"
)

response = llm.complete("Explain quantum computing")
print(response)
```

## Parameters

### Request Parameters

| Parameter | Type | Default | Range | Description |
|-----------|------|---------|-------|-------------|
| `model` | string | required | - | Model ID or "auto" |
| `messages` | array | required | - | Conversation messages |
| `temperature` | float | 1.0 | 0.0-2.0 | Sampling temperature |
| `max_tokens` | int | 256 | 1-4096 | Maximum response tokens |
| `top_p` | float | 1.0 | 0.0-1.0 | Nucleus sampling |
| `stream` | bool | false | - | Stream response |
| `stop` | string/array | null | - | Stop sequences |
| `presence_penalty` | float | 0.0 | -2.0-2.0 | Presence penalty |
| `frequency_penalty` | float | 0.0 | -2.0-2.0 | Frequency penalty |

### Message Format

```json
{
  "role": "system|user|assistant",
  "content": "message text"
}
```

**Roles:**
- `system`: Sets behavior/context
- `user`: User input
- `assistant`: AI response

### Temperature Guide

```
0.0-0.3:  Very focused, deterministic
0.4-0.7:  Balanced creativity (recommended)
0.8-1.2:  Creative, varied
1.3-2.0:  Very creative, experimental
```

## Troubleshooting

### Common Issues

#### Connection Refused

```
Error: Connection refused
```

**Solutions:**
- Check backend is deployed: `./deploy.sh`
- Verify function app is running
- Test health endpoint: `curl https://<app>.azurewebsites.net/api/v1/health`

#### Authentication Failed

```
Error: 401 Unauthorized
```

**Solutions:**
- Verify API key is correct
- Check Authorization header format: `Bearer <key>`
- Regenerate key if compromised

#### Model Not Found

```
Error: model 'xyz' is not supported
```

**Solutions:**
- Use valid model ID: `mixtral-8x7b`, `llama-3-70b`, `phi-3-mini`
- Or use `auto` for automatic selection
- Check available models: `GET /models`

#### Timeout

```
Error: Request timeout
```

**Solutions:**
- Check backend GPU instances are running
- Increase timeout (first request may be slow)
- Try lighter model (`phi-3-mini`)

#### Context Length Exceeded

```
Error: estimated tokens exceed model context length
```

**Solutions:**
- Reduce message length
- Use model with larger context (`mixtral-8x7b`: 32K)
- Truncate conversation history

### Debugging

#### Enable Verbose Logging

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Check Backend Health

```bash
# Health check
curl https://your-app.azurewebsites.net/api/v1/health

# List models
curl https://your-app.azurewebsites.net/api/v1/models \
  -H "Authorization: Bearer your-key"
```

#### View Logs

```bash
# Function app logs
az functionapp log tail --name <function-app> --resource-group <rg>

# Container app logs
az containerapp logs show --name <container-app> --resource-group <rg> --tail 100
```

## Rate Limits & Quotas

### Current Limits

> **Note**: These limits are current as of the last update date at the end of this document. They may be adjusted based on usage patterns and Azure capacity. Refer to the Azure Portal for the most up-to-date limits and quotas.

- **Requests**: No hard limit (Azure Functions scale dynamically)
- **Tokens**: Up to 4096 per request
- **Context**: Model-specific (4K-32K tokens)
- **Concurrent**: Up to 20 GPU instances (configurable)

### Best Practices

- Implement exponential backoff on errors
- Cache responses when possible
- Use appropriate model for task
- Monitor usage in Azure Portal

## Support

### Resources

- **Documentation**: `docs/` directory
- **Quick Start**: `QUICKSTART-FRONTEND.md`
- **Frontend Usage**: `docs/frontend-usage.md`
- **API Spec**: `openapi.json`

### Getting Help

1. Check this guide and other documentation
2. Review Azure Function logs
3. Test with cURL to isolate issues
4. Check GitHub repository issues

### Useful Commands

```bash
# Get all connection details
./setup-frontend-complete.sh

# Launch frontend
./launch-frontend.sh --all

# Deploy backend
./deploy.sh

# View logs
az containerapp logs show --name <app> --resource-group <rg> --tail 100 --follow

# Restart services
az containerapp revision restart --name <app> --resource-group <rg>
```

## Appendix

### OpenAPI Specification

See `openapi.json` for full API specification.

### Environment Variables

When deploying applications:

```bash
export OPENAI_API_KEY="your-api-key"
export OPENAI_API_BASE="https://your-app.azurewebsites.net/api/v1"
```

### Example .env File

```env
# AI Inference Platform Configuration
OPENAI_API_KEY=your-api-key-here
OPENAI_API_BASE=https://your-app.azurewebsites.net/api/v1
DEFAULT_MODEL=mixtral-8x7b
TEMPERATURE=0.7
MAX_TOKENS=256
```

### Performance Tips

1. **Use caching**: 40% cache hit rate for repeated queries
2. **Select appropriate model**: Don't use llama-3-70b for simple tasks
3. **Limit tokens**: Set reasonable `max_tokens`
4. **Batch requests**: Group similar queries
5. **Monitor costs**: Check Azure Portal regularly

---

**Last Updated**: 2024-01-17
**Version**: 1.0
